apiVersion: cr.kanister.io/v1alpha1
kind: Blueprint
metadata:
  name: kafka-blueprint-minio
actions:
  backup:
    outputArtifacts:
      s3Dump:
        keyValue:
          s3path: '{{ .Phases.setupPhase.Output.s3path }}'
          backupDetail: '{{ .Phases.setupPhase.Output.backupDetail }}' 
    phases:
    - func: KubeTask
      name: setupPhase
      args:
        namespace: "{{ .Object.metadata.namespace }}"
        podOverride:
          containers:
          - name: container
            imagePullPolicy: IfNotPresent
          restartPolicy: Never
        image: bullseie/kafka-connect-test-image:1.0.2
        command:
          - bash
          - -o
          - errexit
          - -o
          - pipefail
          - -c
          - |
            mkdir /tmp/config
            export AWS_ACCESS_KEY="xxxxxx"
            export AWS_SECRET_KEY="xxxxxx"
            export REGION="{{ .Profile.Location.Region }}"
            export BUCKET="{{ .Profile.Location.Bucket }}"
            
            export CONNECTORNAME=$HOSTNAME
            S3CONFIG="{{ index .Object.data "minio-s3-sink.properties" | toString }}"
            echo -e "${S3CONFIG}\ns3.region=${REGION}\ns3.bucket.name=${BUCKET}\nname=${CONNECTORNAME}\n" > /tmp/config/s3config.properties
            # S3FOLDER=`cat /tmp/config/s3config.properties | grep "s3.prefix=" | awk -F "=" '{print $2}'`
            # S3_TOPIC_PATH="${S3FOLDER}-{{ .Time | date "2006-01-02T15:04:05" }}"
            # S3_TOPIC_PATH=$(echo "${S3_TOPIC_PATH}" | sed 's/:/-/g' | sed 's/T/t/g')

            sed -i "/^s3.prefix/d" /tmp/config/s3config.properties
            # echo -e "\ns3.prefix=${S3_TOPIC_PATH}\n" >> /tmp/config/s3config.properties
           
            # export S3_PATH="s3://{{ .Profile.Location.Bucket }}/${S3_TOPIC_PATH}"
            export S3_PATH="s3://{{ .Profile.Location.Bucket }}/topics" 
            KAFKACONFIG="{{ index .Object.data "kafka-configuration.properties" | toString }}"
            echo "$KAFKACONFIG" > /tmp/config/kafkaConfig.properties
           
            export TIMEINSECONDS="{{ index .Object.data "timeinSeconds" | toString }}"
            export BOOTSTRAPSERVER=`cat /tmp/config/kafkaConfig.properties | grep "bootstrap.servers=" | awk -F "=" '{print $2}'`
            echo "============ENV variable set====================="
            
            /opt/kafka/bin/connect-standalone.sh /tmp/config/kafkaConfig.properties /tmp/config/s3config.properties & 
            export PID=$!
            
            echo "REGION = $REGION"
            echo "BUCKET = $BUCKET"
            echo "AWS_SECRET_KEY = $AWS_SECRET_KEY"
            echo "AWS_ACCESS_KEY = $AWS_ACCESS_KEY"
            echo "CONNECTORNAME = $CONNECTORNAME"
            echo "S3_PATH = $S3_PATH"
            echo "TIMEINSECONDS = $TIMEINSECONDS"
            echo "BOOTSTRAPSERVER = $BOOTSTRAPSERVER"
            echo "PID = $PID"  
            
            cat /tmp/config/s3config.properties
            cat /tmp/config/kafkaConfig.properties
            
            # script to monitors sink connector backup all topic and stops the connector when lag is zero
            sh monitorsink.sh 

            #sleep 120 
            exit 0
  prerestore:
    inputArtifactNames:
    - s3Dump 
    phases:
    - func: KubeTask
      name: restorePreHookPhase
      args:
        namespace: "{{ .Object.metadata.namespace }}"
        podOverride:
          containers:
          - name: container
            imagePullPolicy: IfNotPresent
        image: bullseie/kafka-connect-test-image:1.0.2 
        command:
        - bash
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |
          mkdir /tmp/config
          echo "Running Restore Prephase" 
          export AWS_ACCESS_KEY="xxxxxx"
          export AWS_SECRET_KEY="xxxxxx"
          export REGION="{{ .Profile.Location.Region }}"
          export BUCKET="{{ .Profile.Location.Bucket }}"

          echo "REGION = $REGION"
          echo "BUCKET = $BUCKET"
          echo "AWS_SECRET_KEY = $AWS_SECRET_KEY"
          echo "AWS_ACCESS_KEY = $AWS_ACCESS_KEY"
          
          KAFKACONFIG="{{ index .Object.data "kafka-configuration.properties" | toString }}"
          echo "$KAFKACONFIG" > /tmp/config/kafkaConfig.properties

          S3CONFIG="{{ index .Object.data "minio-s3-source.properties" | toString }}"
          echo "${S3CONFIG}" > /tmp/config/s3config.properties

          export BOOTSTRAPSERVER=`cat /tmp/config/kafkaConfig.properties | grep "bootstrap.servers=" | awk -F "=" '{print $2}'`

          cat /tmp/config/s3config.properties | grep "topics=" | awk -F "=" '{print $2}' | tr , "\n" > /tmp/config/topics.txt
          
          cat /tmp/config/kafkaConfig.properties
          cat /tmp/config/s3config.properties
          cat /tmp/config/topics.txt
          echo ""
          echo "BOOTSTRAPSERVER  = $BOOTSTRAPSERVER"
          
          while IFS= read -r TOPIC 
          do
              # getting topic name from configuration file
              echo "purging topic $TOPIC"
              # getting retention period as set for the topic
              export RETENTION_PERIOD="$(/opt/kafka/bin/kafka-configs.sh --describe --bootstrap-server "$BOOTSTRAPSERVER" --entity-type topics --entity-name "$TOPIC" --all | grep -m1 retention.ms= | sed 's/[^0-9]*//; s/ .*//')"
              # purging topic by setting retention to 1ms
              /opt/kafka/bin/kafka-configs.sh --bootstrap-server "$BOOTSTRAPSERVER" --entity-type topics --entity-name "$TOPIC" --alter --add-config retention.ms=1
              echo "retention set to 1"
              # Verifying Purging is complete
              export START_OFFSET="$(/opt/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list "$BOOTSTRAPSERVER" --topic "$TOPIC" --time -1)"
              export END_OFFSET="$(/opt/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list "$BOOTSTRAPSERVER" --topic "$TOPIC" --time -2)"
              until [ "$START_OFFSET" = "$END_OFFSET" ]
              do
              echo "purging in process"
              START_OFFSET="$(/opt/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list "$BOOTSTRAPSERVER" --topic "$TOPIC" --time -1)"
              END_OFFSET="$(/opt/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list "$BOOTSTRAPSERVER" --topic "$TOPIC" --time -2)"
              
              echo "START_OFFSET = $START_OFFSET"
              echo "END_OFFSET = $END_OFFSET"

              sleep 2 
              done
              echo "purging complete for topic $TOPIC"
              echo "resetting the retention to previous value"
              # reset the retention period to previous value
              /opt/kafka/bin/kafka-configs.sh --bootstrap-server "$BOOTSTRAPSERVER" --entity-type topics --entity-name "$TOPIC" /opt/kafka/bin/kafka-configs.sh --alter --add-config retention.ms="$RETENTION_PERIOD"
          done < "/tmp/config/topics.txt"
              exit 0
  restore:
    inputArtifactNames:
    - s3Dump
    phases:
    - func: KubeTask
      name: restorePhase
      args:
        namespace: "{{ .Object.metadata.namespace }}"
        podOverride:
          containers:
          - name: container
            imagePullPolicy: IfNotPresent
        image: bullseie/kafka-connect-test-image:1.0.2 
        command:
          - bash
          - -o
          - errexit
          - -o
          - pipefail
          - -c
          - |
            echo "Running restore phase" 
            mkdir /tmp/config
            export AWS_ACCESS_KEY="xxxxxx"
            export AWS_SECRET_KEY="xxxxx"
            export REGION="{{ .Profile.Location.Region }}"
            export BUCKET="{{ .Profile.Location.Bucket }}"
            echo "REGION = $REGION"
            echo "BUCKET = $BUCKET"
            echo "AWS_SECRET_KEY = $AWS_SECRET_KEY"
            echo "AWS_ACCESS_KEY = $AWS_ACCESS_KEY"
            
            export CONNECTORNAME=$HOSTNAME
            S3CONFIG="{{ index .Object.data "minio-s3-source.properties" | toString }}"
            echo -e "${S3CONFIG}\ns3.region=${REGION}\ns3.bucket.name=${BUCKET}\nname=${CONNECTORNAME}\n" > /tmp/config/s3config.properties
            sed -i "/^s3.prefix/d" /tmp/config/s3config.properties

            S3_PATH="{{ .ArtifactsIn.s3Dump.KeyValue.s3path }}"
            echo $S3_PATH

            export TOPICS_DIR="$(echo $S3_PATH | awk -F "/" '{print $(NF)}')"
            echo "TOPICS_DIR = $TOPICS_DIR" 
            echo -e "\ns3.prefix=${TOPICS_DIR}\n" >> /tmp/config/s3config.properties

            KAFKACONFIG="{{ index .Object.data "kafka-configuration.properties" | toString }}"
            echo "$KAFKACONFIG" > /tmp/config/kafkaConfig.properties

            TOPIC_DETAIL="{{ .ArtifactsIn.s3Dump.KeyValue.backupDetail }}"
            echo "TOPIC_DETAIL = $TOPIC_DETAIL" 
            export BOOTSTRAPSERVER=`cat /tmp/config/kafkaConfig.properties | grep "bootstrap.servers=" | awk -F "=" '{print $2}'`
            export TOPIC_LIST=`cat /tmp/config/s3config.properties | grep "topics=" | awk -F "=" '{print $2}'`
            echo "TOPIC_LIST = TOPIC_LIST"

            echo "============ENV variable set====================="

            cat /tmp/config/s3config.properties
            cat /tmp/config/kafkaConfig.properties
            
            # start kafka source connector
            /opt/kafka/bin/connect-standalone.sh /tmp/config/kafkaConfig.properties /tmp/config/s3config.properties &
            export PID=$!
            # script to monitors source connector to restore all topic and stops the connector successfully
            sh monitorsource.sh
            exit 0

  delete:
    inputArtifactNames:
    - s3Dump
    phases:
    - func: KubeTask
      name: deleteFromBlobStore
      args:
        namespace: "{{ .Object.metadata.namespace }}"
        podOverride:
          containers:
          - name: container
            imagePullPolicy: IfNotPresent
        image: bullseie/kafka-connect-test-image:1.0.2 
        command:
        - bash
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |
          mkdir /tmp/config
          echo "Running deleteFromBlobStore phase"
          export AWS_ACCESS_KEY="xxxxxx"
          export AWS_SECRET_KEY="xxxxxx"
          export REGION="{{ .Profile.Location.Region }}"
          export BUCKET="{{ .Profile.Location.Bucket }}"
          
          echo "REGION = $REGION"
          echo "BUCKET = $BUCKET"
          echo "AWS_SECRET_KEY = $AWS_SECRET_KEY"
          echo "AWS_ACCESS_KEY = $AWS_ACCESS_KEY"
          
          export CONNECTORNAME=$HOSTNAME 
          S3CONFIG="{{ index .Object.data "minio-s3-source.properties" | toString }}"
          echo "$S3CONFIG" > /tmp/config/s3config.properties
          export OBJECT_STORE_URL=`cat /tmp/config/s3config.properties | grep "store.url=" | awk -F "=" '{print $2}'`
          
          echo "OBJECT_STORE_URL - $OBJECT_STORE_URL" 
          export S3PATH="{{ .ArtifactsIn.s3Dump.KeyValue.s3path }}"
          echo "S3PATH - $S3PATH"

          export DELETE_PATH="$(echo $S3PATH | awk -F "//" '{print $2}')"
          echo "DELETE_PATH - $DELETE_PATH"

          cat /tmp/config/s3config.properties
          
          echo "OBJECT_STORE_URL - $OBJECT_STORE_URL"
          echo "DELETE_PATH - $DELETE_PATH"
          echo "/opt/kafka/mc alias set minio ${OBJECT_STORE_URL} ${AWS_ACCESS_KEY} ${AWS_SECRET_KEY}"
          echo " /opt/kafka/mc rm --recursive --force minio/${DELETE_PATH}"

          /opt/kafka/mc alias set minio ${OBJECT_STORE_URL} ${AWS_ACCESS_KEY} ${AWS_SECRET_KEY}
          /opt/kafka/mc rm --recursive --force minio/${DELETE_PATH}

          exit 0
